{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Training for Climbing Grade Prediction\n",
    "This notebook trains a Convolutional Neural Network to predict climbing grades from hold placements and route features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('.')  # Add current directory to Python path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Simple matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Custom Modules\n",
    "try:\n",
    "    from src.data.preprocessing import load_data, create_boulder_angle_dataframe\n",
    "    from src.data.analysis import analyze_and_clean_data\n",
    "    from src.models.preprocessing import create_train_test_split\n",
    "    from src.models.cnn_model import create_cnn_model\n",
    "    from src.visualization.model_plots import plot_training_history\n",
    "    from src.features.grade_conversion import difficulty_to_vgrade\n",
    "    print(\"‚úì Custom modules loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and Prepare Data\n",
    "print(\"Loading climbing data...\")\n",
    "data_path = \"data/raw/climbs.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    # Load raw data\n",
    "    df = load_data(data_path)\n",
    "    print(f\"‚úì Loaded {len(df)} climbing routes\")\n",
    "    \n",
    "    # Clean the data\n",
    "    print(\"\\nCleaning data...\")\n",
    "    cleaned_df, profile, stats = analyze_and_clean_data(df, \n",
    "                                                       upper_percentile=95, \n",
    "                                                       save_cleaned=False)\n",
    "    \n",
    "    if cleaned_df is not None:\n",
    "        print(f\"‚úì Data cleaned: {len(cleaned_df)} routes remain\")\n",
    "        \n",
    "        # Create boulder angle dataframe\n",
    "        print(\"\\nCreating angle-specific dataset...\")\n",
    "        boulder_angles_df = create_boulder_angle_dataframe(cleaned_df, min_ascents=2)\n",
    "        print(f\"‚úì Created {len(boulder_angles_df)} angle-specific entries\")\n",
    "        \n",
    "        # Show sample of the data\n",
    "        print(\"\\nSample of processed data:\")\n",
    "        display(boulder_angles_df[['name', 'angle', 'grade', 'ascents', 'hold_count']].head(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to clean data\")\n",
    "else:\n",
    "    print(f\"‚ùå Data file not found at {data_path}\")\n",
    "    print(\"Please make sure your climbs.csv file is in the data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Statistics and Visualization\n",
    "if 'boulder_angles_df' in locals():\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"Total entries: {len(boulder_angles_df)}\")\n",
    "    print(f\"Unique problems: {boulder_angles_df['name'].nunique()}\")\n",
    "    print(f\"Grade range: {boulder_angles_df['grade'].min():.1f} - {boulder_angles_df['grade'].max():.1f}\")\n",
    "    print(f\"Angle range: {boulder_angles_df['angle'].min()}¬∞ - {boulder_angles_df['angle'].max()}¬∞\")\n",
    "    print(f\"Hold count range: {boulder_angles_df['hold_count'].min()} - {boulder_angles_df['hold_count'].max()}\")\n",
    "    \n",
    "    # Visualize grade distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Grade distribution\n",
    "    axes[0,0].hist(boulder_angles_df['grade'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('Grade Distribution (Difficulty Scale)')\n",
    "    axes[0,0].set_xlabel('Difficulty Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Angle distribution\n",
    "    axes[0,1].hist(boulder_angles_df['angle'], bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[0,1].set_title('Angle Distribution')\n",
    "    axes[0,1].set_xlabel('Board Angle (degrees)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hold count distribution\n",
    "    axes[1,0].hist(boulder_angles_df['hold_count'], bins=25, alpha=0.7, edgecolor='black', color='green')\n",
    "    axes[1,0].set_title('Hold Count Distribution')\n",
    "    axes[1,0].set_xlabel('Number of Holds')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Grade vs Hold Count scatter\n",
    "    axes[1,1].scatter(boulder_angles_df['hold_count'], boulder_angles_df['grade'], alpha=0.5)\n",
    "    axes[1,1].set_title('Grade vs Hold Count')\n",
    "    axes[1,1].set_xlabel('Number of Holds')\n",
    "    axes[1,1].set_ylabel('Difficulty Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show V-grade distribution\n",
    "    v_grades = [difficulty_to_vgrade(grade) for grade in boulder_angles_df['grade']]\n",
    "    v_grade_counts = pd.Series(v_grades).value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    v_grade_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution by V-Grade')\n",
    "    plt.xlabel('V-Grade')\n",
    "    plt.ylabel('Number of Problems')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Train-Test Split\n",
    "if 'boulder_angles_df' in locals():\n",
    "    print(\"Creating train-test split...\")\n",
    "    \n",
    "    # Create the split (ensures same boulder problems don't appear in both sets)\n",
    "    X_train, X_test, y_train, y_test = create_train_test_split(boulder_angles_df, \n",
    "                                                              test_size=0.2, \n",
    "                                                              random_state=42)\n",
    "    \n",
    "    print(f\"\\n‚úì Data split complete:\")\n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    print(f\"Training problems: {X_train['name'].nunique()}\")\n",
    "    print(f\"Test problems: {X_test['name'].nunique()}\")\n",
    "    \n",
    "    # Check grade distribution in splits\n",
    "    train_v_grades = [difficulty_to_vgrade(grade) for grade in y_train]\n",
    "    test_v_grades = [difficulty_to_vgrade(grade) for grade in y_test]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    pd.Series(train_v_grades).value_counts().sort_index().plot(kind='bar', ax=ax1, title='Training Set V-Grades')\n",
    "    pd.Series(test_v_grades).value_counts().sort_index().plot(kind='bar', ax=ax2, title='Test Set V-Grades')\n",
    "    \n",
    "    ax1.set_ylabel('Count')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå No data available for splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train the CNN Model\n",
    "if all(var in locals() for var in ['X_train', 'X_test', 'y_train', 'y_test']):\n",
    "    print(\"Starting CNN model training...\")\n",
    "    print(\"This may take several minutes depending on your hardware.\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        model, history, metrics = create_cnn_model(\n",
    "            boulder_angles_df, \n",
    "            X_train, X_test, \n",
    "            y_train, y_test\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üéâ MODEL TRAINING COMPLETE!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Display metrics\n",
    "        print(\"\\nFinal Model Performance:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "            \n",
    "        training_complete = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during model training: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"- Make sure TensorFlow is properly installed\")\n",
    "        print(\"- Check if you have enough memory (reduce batch size if needed)\")\n",
    "        print(\"- Verify your data has the required columns\")\n",
    "        training_complete = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Training data not available. Please run the previous cells first.\")\n",
    "    training_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualize Training Results\n",
    "if 'training_complete' in locals() and training_complete and 'history' in locals():\n",
    "    print(\"Plotting training history...\")\n",
    "    \n",
    "    # Plot training history\n",
    "    try:\n",
    "        fig = plot_training_history(history)\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional detailed plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss over time\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        axes[0,0].plot(epochs, history.history['loss'], 'b-', label='Training Loss')\n",
    "        axes[0,0].plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "        axes[0,0].set_title('Training and Validation Loss')\n",
    "        axes[0,0].set_xlabel('Epochs')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE over time\n",
    "        axes[0,1].plot(epochs, history.history['mean_absolute_error'], 'b-', label='Training MAE')\n",
    "        axes[0,1].plot(epochs, history.history['val_mean_absolute_error'], 'r-', label='Validation MAE')\n",
    "        axes[0,1].set_title('Training and Validation MAE')\n",
    "        axes[0,1].set_xlabel('Epochs')\n",
    "        axes[0,1].set_ylabel('Mean Absolute Error')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate (if available)\n",
    "        if 'lr' in history.history:\n",
    "            axes[1,0].plot(epochs, history.history['lr'], 'g-')\n",
    "            axes[1,0].set_title('Learning Rate Schedule')\n",
    "            axes[1,0].set_xlabel('Epochs')\n",
    "            axes[1,0].set_ylabel('Learning Rate')\n",
    "            axes[1,0].set_yscale('log')\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1,0].text(0.5, 0.5, 'Learning Rate\\nNot Recorded', \n",
    "                          ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "            axes[1,0].set_title('Learning Rate Schedule')\n",
    "        \n",
    "        # Model performance summary\n",
    "        axes[1,1].axis('off')\n",
    "        summary_text = \"Model Performance Summary\\n\\n\"\n",
    "        for metric_name, value in metrics.items():\n",
    "            summary_text += f\"{metric_name.replace('_', ' ').title()}: {value:.4f}\\n\"\n",
    "        \n",
    "        axes[1,1].text(0.1, 0.9, summary_text, transform=axes[1,1].transAxes, \n",
    "                      fontsize=12, verticalalignment='top', \n",
    "                      bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No training results to display. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Make Predictions and Analyze Results\n",
    "if 'training_complete' in locals() and training_complete and 'model' in locals():\n",
    "    print(\"Making predictions on test set...\")\n",
    "    \n",
    "    try:\n",
    "        # Get model predictions\n",
    "        from src.models.cnn_model import create_multichannel_grid, encode_hold_types\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        # Prepare test data\n",
    "        feature_cols = ['angle', 'hold_count', 'ascents']\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Scale features (fit on training, transform test)\n",
    "        X_train_scaled = scaler.fit_transform(X_train[feature_cols])\n",
    "        X_test_scaled = scaler.transform(X_test[feature_cols])\n",
    "        \n",
    "        # Create grids and hold type encodings\n",
    "        test_grids = np.array([create_multichannel_grid(p) for p in X_test['placements']])\n",
    "        test_hold_types = np.array([encode_hold_types(p) for p in X_test['placements']])\n",
    "        test_features = np.hstack((X_test_scaled, test_hold_types))\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict([test_grids, test_features])\n",
    "        \n",
    "        # Convert to V-grades for interpretation\n",
    "        actual_v_grades = [difficulty_to_vgrade(grade) for grade in y_test]\n",
    "        predicted_v_grades = [difficulty_to_vgrade(pred[0]) for pred in predictions]\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'Actual_Grade': y_test.values,\n",
    "            'Predicted_Grade': predictions.flatten(),\n",
    "            'Actual_V_Grade': actual_v_grades,\n",
    "            'Predicted_V_Grade': predicted_v_grades,\n",
    "            'Error': y_test.values - predictions.flatten(),\n",
    "            'Abs_Error': np.abs(y_test.values - predictions.flatten())\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úì Predictions completed for {len(results_df)} test samples\")\n",
    "        \n",
    "        # Show sample predictions\n",
    "        print(\"\\nSample Predictions:\")\n",
    "        display(results_df[['Actual_V_Grade', 'Predicted_V_Grade', 'Abs_Error']].head(10))\n",
    "        \n",
    "        # Plot predictions vs actual\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Scatter plot of predictions vs actual\n",
    "        axes[0].scatter(y_test, predictions, alpha=0.6)\n",
    "        axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        axes[0].set_xlabel('Actual Grade')\n",
    "        axes[0].set_ylabel('Predicted Grade')\n",
    "        axes[0].set_title('Predictions vs Actual Grades')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error distribution\n",
    "        axes[1].hist(results_df['Error'], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "        axes[1].set_xlabel('Prediction Error')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Distribution of Prediction Errors')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # V-grade accuracy analysis\n",
    "        exact_matches = sum(a == p for a, p in zip(actual_v_grades, predicted_v_grades))\n",
    "        within_one = sum(abs(actual_v_grades.index(a) - predicted_v_grades.index(p)) <= 1 \n",
    "                        for a, p in zip(actual_v_grades, predicted_v_grades) \n",
    "                        if a in predicted_v_grades and p in actual_v_grades)\n",
    "        \n",
    "        print(f\"\\nV-Grade Prediction Accuracy:\")\n",
    "        print(f\"Exact matches: {exact_matches}/{len(actual_v_grades)} ({exact_matches/len(actual_v_grades)*100:.1f}%)\")\n",
    "        print(f\"Within ¬±1 grade: {within_one}/{len(actual_v_grades)} ({within_one/len(actual_v_grades)*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"No trained model available for predictions. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Model and Results\n",
    "if 'training_complete' in locals() and training_complete and 'model' in locals():\n",
    "    print(\"Saving model and results...\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save the model\n",
    "        model_path = 'models/climbing_cnn_model.h5'\n",
    "        model.save(model_path)\n",
    "        print(f\"‚úì Model saved to {model_path}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_path = 'reports/model_metrics.txt'\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            f.write(\"CNN Model Evaluation Metrics\\n\")\n",
    "            f.write(\"=\" * 30 + \"\\n\")\n",
    "            for key, value in metrics.items():\n",
    "                f.write(f\"{key}: {value:.4f}\\n\")\n",
    "            \n",
    "            # Add training info\n",
    "            f.write(f\"\\nTraining Information:\\n\")\n",
    "            f.write(f\"Training samples: {len(X_train)}\\n\")\n",
    "            f.write(f\"Test samples: {len(X_test)}\\n\")\n",
    "            f.write(f\"Total epochs: {len(history.history['loss'])}\\n\")\n",
    "            \n",
    "        print(f\"‚úì Metrics saved to {metrics_path}\")\n",
    "        \n",
    "        # Save predictions if available\n",
    "        if 'results_df' in locals():\n",
    "            predictions_path = 'reports/test_predictions.csv'\n",
    "            results_df.to_csv(predictions_path, index=False)\n",
    "            print(f\"‚úì Test predictions saved to {predictions_path}\")\n",
    "        \n",
    "        print(\"\\nüéâ All results saved successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No trained model to save. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Loads and preprocesses climbing route data\n",
    "2. ‚úÖ Creates train/test splits ensuring no data leakage\n",
    "3. ‚úÖ Trains a CNN model to predict climbing grades\n",
    "4. ‚úÖ Evaluates model performance with multiple metrics\n",
    "5. ‚úÖ Visualizes training progress and results\n",
    "6. ‚úÖ Saves the trained model and predictions\n",
    "\n",
    "### Next Steps:\n",
    "- Try different model architectures\n",
    "- Experiment with hyperparameter tuning\n",
    "- Add more features (route setter, popularity, etc.)\n",
    "- Compare with simpler baseline models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
